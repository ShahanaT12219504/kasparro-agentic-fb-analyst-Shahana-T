import json
from src.utils.prompts_loader import load_prompt
from src.utils.logging_utils import write_json_log

class InsightAgent:
    """
    The Insight Agent reads the numeric summary generated by the Data Agent
    and produces *hypotheses* that explain trends in performance.

    This agent does NOT calculate metrics â€” it only interprets them using the LLM.
    It converts raw numbers into structured reasoning.
    """

    def __init__(self, llm_client):
        self.llm_client = llm_client
        self.prompt_template = load_prompt("insight_agent")

    def generate_hypotheses(self, data_summary: dict, user_query: str) -> list:
        """
        Produces a list of hypotheses based on the numeric summary.

        Steps:
        1. Insert the numeric summary + user query into the prompt
        2. Ask the LLM to produce a JSON list of hypotheses
        3. Validate JSON output
        """

        prompt = (
            self.prompt_template
            + "\n\nUSER_QUERY:\n"
            + user_query
            + "\n\nDATA_SUMMARY:\n"
            + json.dumps(data_summary, indent=2)
        )

        # Log what is being sent to LLM
        write_json_log("insight_prompt_sent", {"summary_keys": list(data_summary.keys())})

        # Generate hypotheses
        response = self.llm_client.generate(
            prompt=prompt,
            temperature=0.2
        )

        # Log raw response
        write_json_log("insight_raw_response", {"response": response})

        # Parse JSON safely
        try:
            hypotheses = json.loads(response)
        except Exception:
            fix_prompt = (
                prompt
                + "\n\nYour previous output was not valid JSON. Return ONLY a JSON list of hypotheses."
            )
            response = self.llm_client.generate(prompt=fix_prompt, temperature=0.2)
            hypotheses = json.loads(response)

        # Final log
        write_json_log("insight_final_hypotheses", {"count": len(hypotheses)})

        return hypotheses
